{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZhYB-cisobs",
        "outputId": "924dc1ae-3d49-4920-9dd5-530d7d8933a9"
      },
      "outputs": [],
      "source": [
        "# Install as needed\n",
        "#!pip install sentencepiece torch pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvR_X_oDsa-R"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uxM2k3Z3sa-T"
      },
      "outputs": [],
      "source": [
        "#Check if GPU is available\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XD4Qh-Mrsa-T",
        "outputId": "a912c5a3-9952-48c8-dd38-4bfa905c72e6"
      },
      "outputs": [],
      "source": [
        "import_size = 500000\n",
        "\n",
        "with open('raw_corpus.txt', 'r', encoding=\"utf-8-sig\") as f:\n",
        "  data = f.readlines()[:import_size]\n",
        "\n",
        "data = [line.rstrip() for line in data]\n",
        "dataset = pd.DataFrame(data, columns=['text'])\n",
        "dataset = dataset[dataset[\"text\"].str.strip().astype(bool)]\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwHAlPsWsa-U",
        "outputId": "75cf9d8e-c0e2-4970-85b5-4e8050f92c71"
      },
      "outputs": [],
      "source": [
        "dataset.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4dAB5o_sa-U"
      },
      "outputs": [],
      "source": [
        "data = ' '.join(dataset[\"text\"].tolist())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Components definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrfQUfVOsa-U"
      },
      "outputs": [],
      "source": [
        "#Defines a token embedding layer, essentially replaces each token index in the input with its learned vector representation.\n",
        "class Embedding(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dimension\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p3P9NIgsa-U"
      },
      "outputs": [],
      "source": [
        "#Applies the positional encoding to the input embeddings\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.positional_encoding = self.create_positional_encoding()\n",
        "    # Creates a sinusoidal positional encoding matrix of size (max_sequence_length, embedding_dimension).\n",
        "    def create_positional_encoding(self):\n",
        "        positional_encoding = np.zeros((self.max_sequence_length, self.embedding_dimension))\n",
        "        for pos in range(self.max_sequence_length):\n",
        "            for i in range(0, self.embedding_dimension, 2):\n",
        "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.embedding_dimension)))\n",
        "                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / self.embedding_dimension)))\n",
        "        return torch.from_numpy(positional_encoding).float().to(get_device())\n",
        "    #Applies the positional encoding to the token embeddings.\n",
        "    def forward(self, x):\n",
        "        return x + self.positional_encoding[:x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aTiIn4asa-V"
      },
      "outputs": [],
      "source": [
        "#Defines a PyTorch module for a self-attention layer that computes attention weights between tokens, with masked tokens excluded from the calculation of the weights.\n",
        "class MaskedSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, head_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = head_dimension\n",
        "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "    \n",
        "    #Applies the self attention\n",
        "    def forward(self, x, mask):\n",
        "        # x is the input\n",
        "        # The query, key and value are created by linear transformation of the input\n",
        "        query = self.query_layer(x)\n",
        "        key = self.key_layer(x)\n",
        "        value = self.value_layer(x)\n",
        "\n",
        "        # Calculate the attention weights.\n",
        "        # Attention weights are computed by matrix multiplication of query and transposed key\n",
        "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "        # The attention weights are scaled to avoid issues with small gradients\n",
        "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
        "\n",
        "        # A mask is applied to the attention weights to ensure that the model does not pay attention to certain tokens in the sequence\n",
        "        # Mask values are 0 (token is masked) or 1 (token is not masked)\n",
        "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
        "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # The softmax function is applied to the attention weights to convert them into a probabilistic distribution\n",
        "        # Tokens with high attention scores are emphasized, while those with low scores are de-emphasized\n",
        "        attention_scores = self.softmax(attention_weights)\n",
        "        return torch.bmm(attention_scores, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NhSSrSAsa-W"
      },
      "outputs": [],
      "source": [
        "#Module that combines the outputs of multiple self attention modules to learn different relationships between each token\n",
        "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, number_of_heads):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Set the parameters for multi-headed self attention\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dimension = embedding_dimension // number_of_heads\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        # Create the self attention modules\n",
        "        self.self_attentions = torch.nn.ModuleList(\n",
        "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
        "\n",
        "        # Create a linear layer to combine the outputs of the self attention modules\n",
        "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
        "\n",
        "    \n",
        "    #Apply multi-headed self attention to the input.\n",
        "    # mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
        "    def forward(self, x, mask):\n",
        "        # Compute the self attention for each head\n",
        "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
        "\n",
        "        # Concatenate the self attention outputs\n",
        "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
        "\n",
        "        # Apply the output layer to the concatenated self attention outputs\n",
        "        return self.output_layer(concatenated_self_attention_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb-EV21Esa-W"
      },
      "outputs": [],
      "source": [
        "# Defines a PyTorch module that implements a feedforward layer.\n",
        "# A feedforward layer is a fully connected neural network layer that applies the Rectified Linear Unit (ReLU) activation function.\n",
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "\n",
        "        # Implements the first linear layer with embedding_dimension inputs and feed_forward_dimension outputs.\n",
        "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
        "        \n",
        "        # Implements the second linear layer with feed_forward_dimension inputs and embedding_dimension outputs.\n",
        "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
        "\n",
        "    #Computes the output of the feedforward layer given an input tensor x.\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(torch.relu(self.linear_1(x)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc7GHfeTsa-X"
      },
      "outputs": [],
      "source": [
        "#A PyTorch module for a single layer of a decoder.\n",
        "#A decoder layer comprises of a multi-headed self-attention layer, followed by a feedforward network layer \n",
        "#and a layer normalization, with dropout applied after the feedforward layer.\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # initialize layers\n",
        "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
        "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "    #Applies the decoder layer\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        # First Layer normalization\n",
        "        normalized_x = self.layer_normalization_1(x)\n",
        "\n",
        "        # Multi headed self attention\n",
        "        self_attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
        "\n",
        "        # Residual connection\n",
        "        normalized_residual = x + self_attention_output\n",
        "\n",
        "        # second Layer normalization\n",
        "        normalized_residual_output = self.layer_normalization_2(normalized_residual)\n",
        "\n",
        "        # Feed forward\n",
        "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
        "\n",
        "        # Dropout\n",
        "        if self.training:\n",
        "            feed_forward_output = self.dropout(feed_forward_output)\n",
        "\n",
        "        # Residual output\n",
        "        return normalized_residual + feed_forward_output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZhDKvENsa-X"
      },
      "outputs": [],
      "source": [
        "# This module creates a stack of decoder layers for the decoder model.\n",
        "class DecoderStack(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dimension,\n",
        "            number_of_layers,\n",
        "            number_of_heads,\n",
        "            feed_forward_dimension,\n",
        "            dropout_rate,\n",
        "            max_sequence_length\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "        self.feed_forward_dimension = feed_forward_dimension\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "        # Create the encoder layers\n",
        "        self.encoder_layers = torch.nn.ModuleList(\n",
        "            [Decoder(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
        "             range(number_of_layers)])\n",
        "\n",
        "    # the input sequence is passed through the layers sequentially\n",
        "    def forward(self, x, mask):\n",
        "        decoder_outputs = x\n",
        "        \n",
        "        for decoder_layer in self.encoder_layers:\n",
        "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
        "\n",
        "        return decoder_outputs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The \"LMHead\" Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI_ZHtdVsa-X"
      },
      "outputs": [],
      "source": [
        "# PyTorch module for the language model head.\n",
        "# The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
        "class LMHead(torch.nn.Module):\n",
        "    def __init__(self, embedding_dimension, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # Define a linear layer\n",
        "        self.linear = torch.nn.Linear(embedding_dimension, vocab_size)\n",
        "    \n",
        "    # Compute the linear layer\n",
        "    def forward(self, x):\n",
        "        # Linear output dimensions\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MHqp-Aesa-Y"
      },
      "outputs": [],
      "source": [
        "# Defines the LanguageModel\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,  # The number of tokens in the vocabulary\n",
        "            max_sequence_length=128,  # The maximum sequence length to use for attention\n",
        "            embedding_dimension=256,  # The dimension of the token embeddings\n",
        "            number_of_layers=3,  # The number of decoder layers to use\n",
        "            number_of_heads=2,  # The number of attention heads to use\n",
        "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
        "            dropout_rate=0.1  # The dropout rate to use\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.number_of_heads = number_of_heads\n",
        "\n",
        "        # If feed_forward_dimension is not set, use 4 times the embedding_dimension as in GPT-2\n",
        "        if feed_forward_dimension is None:\n",
        "            self.feed_forward_dimension = embedding_dimension * 4\n",
        "        else:\n",
        "            self.feed_forward_dimension = feed_forward_dimension\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.token_embedding = Embedding(embedding_dimension, vocab_size)\n",
        "\n",
        "        # Positional encoding layer\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
        "\n",
        "        # Normalization layer\n",
        "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
        "\n",
        "        # Decoder stack\n",
        "        self.decoder = DecoderStack(\n",
        "            embedding_dimension=embedding_dimension,\n",
        "            number_of_layers=number_of_layers,\n",
        "            number_of_heads=number_of_heads,\n",
        "            feed_forward_dimension=self.feed_forward_dimension,\n",
        "            dropout_rate=dropout_rate,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )\n",
        "\n",
        "        # Language model head\n",
        "        self.lm_head = LMHead(embedding_dimension, vocab_size)\n",
        "\n",
        "    # Define the forward pass for the model\n",
        "    def forward(self, x, mask):\n",
        "        # Compute the token embeddings\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "\n",
        "        # Compute the positional encoding\n",
        "        positional_encoding = self.positional_encoding(token_embeddings)\n",
        "\n",
        "        # Post embedding layer normalization\n",
        "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
        "\n",
        "        # Feed the data through the decoder stack\n",
        "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
        "\n",
        "        # Feed the output through the language model head\n",
        "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
        "\n",
        "        # Return the output of the language model head\n",
        "        return lm_head_outputs\n",
        "    \n",
        "    # Save a checkpoint of the model\n",
        "    def save_checkpoint(self, path):\n",
        "        print(f'Saving checkpoint {path}')\n",
        "        torch.save({\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'max_sequence_length': self.max_sequence_length,\n",
        "            'embedding_dimension': self.embedding_dimension,\n",
        "            'number_of_layers': self.number_of_layers,\n",
        "            'number_of_heads': self.number_of_heads,\n",
        "            'feed_forward_dimension': self.feed_forward_dimension,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'model_state_dict': self.state_dict()\n",
        "        }, path)\n",
        "\n",
        "    # Loads a checkpoint of the model\n",
        "    @staticmethod\n",
        "    def load_checkpoint(path) -> 'LanguageModel':\n",
        "        checkpoint = torch.load(path)\n",
        "        model = LanguageModel(\n",
        "            vocab_size=checkpoint['vocab_size'],\n",
        "            max_sequence_length=checkpoint['max_sequence_length'],\n",
        "            embedding_dimension=checkpoint['embedding_dimension'],\n",
        "            number_of_layers=checkpoint['number_of_layers'],\n",
        "            number_of_heads=checkpoint['number_of_heads'],\n",
        "            feed_forward_dimension=checkpoint['feed_forward_dimension'],\n",
        "            dropout_rate=checkpoint['dropout_rate']\n",
        "        )\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        return model.to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd4-w4Qjsa-Y"
      },
      "outputs": [],
      "source": [
        "#This PyTorch module serves as a wrapper for a GPT model, enabling it to function in an autoregressive manner.\n",
        "class AutoregressiveWrapper(torch.nn.Module):\n",
        "    def __init__(self, s_model):\n",
        "        super().__init__()\n",
        "        self.model = s_model\n",
        "        self.max_sequence_length = self.model.max_sequence_length\n",
        "\n",
        "    #Autoregressive forward pass\n",
        "    def forward(self, x, mask):\n",
        "        inp, target = x[:, :-1], x[:, 1:]\n",
        "        mask = mask[:, :-1]\n",
        "\n",
        "        output = self.model(inp, mask)\n",
        "        return output, target\n",
        "\n",
        "    #Calculate the token probabilities for the next token in the sequence.\n",
        "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
        "        logits = self.model(x, mask)[:, -1]\n",
        "\n",
        "        # Apply temperature\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # Apply the softmax\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        self.model.save_checkpoint(path)\n",
        "\n",
        "    #Load a checkpoint from a file.\n",
        "    @staticmethod\n",
        "    def load_checkpoint(path) -> 'AutoregressiveWrapper':\n",
        "        model = LanguageModel.load_checkpoint(path)\n",
        "        return AutoregressiveWrapper(model).to(get_device())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K87CJVU9sa-Y"
      },
      "outputs": [],
      "source": [
        "#This class uses the tokenizer made with SentencePiece, trained in the tokenizer notebook\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(\"spanish_lm.model\")\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.sp.Encode(text)\n",
        "\n",
        "    def character_to_token(self, character):\n",
        "        return self.sp.Encode(character)\n",
        "\n",
        "    def token_to_character(self, token):\n",
        "        return self.sp.Decode(token)\n",
        "\n",
        "    def size(self):\n",
        "        return self.sp.GetPieceSize()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeSkhE0zsa-Z"
      },
      "outputs": [],
      "source": [
        "#Trainer\n",
        "class Trainer:\n",
        "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        if optimizer is None:\n",
        "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        else:\n",
        "            self.optimizer = optimizer\n",
        "            \n",
        "\n",
        "    #Trains the language model on the given data for the specified number of epochs.\n",
        "    def train(self, data: List[str], epochs, batch_size):\n",
        "        loss_per_epoch = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "\n",
        "            # Shuffle the sequences\n",
        "            random.shuffle(data)\n",
        "\n",
        "            # Create batches of sequences and their respective mask.\n",
        "            batches = []\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
        "\n",
        "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
        "                mask_tensor = torch.ones_like(sequence_tensor)\n",
        "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')[1]] = 0\n",
        "\n",
        "                batches.append((sequence_tensor, mask_tensor))\n",
        "\n",
        "            # Train the model on each batch\n",
        "            for batch in batches:\n",
        "                self.model.train()\n",
        "\n",
        "                # Create the input and mask tensors\n",
        "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
        "\n",
        "                for i, input_entry in enumerate(batch[0]):\n",
        "                    input_tensor[i] = input_entry\n",
        "\n",
        "                for i, mask_entry in enumerate(batch[1]):\n",
        "                    mask_tensor[i] = mask_entry\n",
        "\n",
        "                # Compute the model output\n",
        "                model_output, target = self.model.forward(\n",
        "                    x=input_tensor.to(get_device()),\n",
        "                    mask=mask_tensor.to(get_device())\n",
        "                )\n",
        "\n",
        "                # Compute the losses\n",
        "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
        "\n",
        "                # Backpropagate the loss.\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the gradients. This is used to prevent exploding gradients.\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "\n",
        "                # Update the model parameters.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Reset the gradients.\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            # Print the loss\n",
        "            epoch_loss = np.average(losses)\n",
        "            loss_per_epoch.append(epoch_loss)\n",
        "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
        "\n",
        "        return loss_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t_LnZphsa-Z"
      },
      "outputs": [],
      "source": [
        "def pad_left(sequence, final_length, padding_token):\n",
        "    return [padding_token] * (final_length - len(sequence)) + sequence"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Language Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeY8Wbflsa-Z"
      },
      "outputs": [],
      "source": [
        "# Defines a Generator class, which takes a language model and a tokenizer as input to generate its output \n",
        "class Generator:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model,\n",
        "            tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    # Converts the input prompt to a tensor and computes the corresponding mask tensor.\n",
        "    def generate(\n",
        "            self,\n",
        "            generated_sentence_length: int = 20,\n",
        "            prompt: str = None,\n",
        "            temperature: float = 1.0,\n",
        "            last_token_position: int = None,\n",
        "            padding_token: int = 0): #0\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        if prompt is None:\n",
        "            start_tokens = [self.tokenizer.character_to_token(padding_token)[1]]\n",
        "        else:\n",
        "            start_tokens = self.tokenizer.tokenize(prompt)\n",
        "\n",
        "        input_tensor = torch.tensor(\n",
        "            pad_left(\n",
        "                sequence=start_tokens,\n",
        "                final_length=self.model.max_sequence_length + 1,\n",
        "                padding_token=padding_token\n",
        "            ),\n",
        "            dtype=torch.long\n",
        "        ).to(get_device())\n",
        "\n",
        "        num_dims = len(input_tensor.shape)\n",
        "\n",
        "        if num_dims == 1:\n",
        "            input_tensor = input_tensor[None, :]\n",
        "\n",
        "        out = input_tensor\n",
        "\n",
        "        for _ in range(generated_sentence_length):\n",
        "            x = out[:, -self.model.max_sequence_length:]\n",
        "\n",
        "            mask = torch.ones_like(x)\n",
        "            mask[x == padding_token] = 0\n",
        "\n",
        "            # Compute the next token probabilities\n",
        "            next_token_probabilities = self.model.next_token_probabilities(\n",
        "                x=x,\n",
        "                temperature=temperature,\n",
        "                mask=mask\n",
        "            )\n",
        "\n",
        "            # Sample the next token from the probability distribution\n",
        "            prediction = torch.multinomial(next_token_probabilities, num_samples=1)\n",
        "\n",
        "            # Append the next token to the output\n",
        "            out = torch.cat([out, prediction], dim=1)\n",
        "\n",
        "            if last_token_position is not None and prediction == last_token_position:\n",
        "                break\n",
        "\n",
        "        generated_tokens = out[0].tolist()\n",
        "\n",
        "        return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y12ZVcUjsa-Z"
      },
      "outputs": [],
      "source": [
        "def generate_training_sequences(max_sequence_length, tokenized_training_data):\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
        "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yen7oS-8sa-a"
      },
      "outputs": [],
      "source": [
        "def preprocess_training_data(max_sequence_length, tokenizer, training_data):\n",
        "    # Tokenize the training data\n",
        "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
        "\n",
        "    for _ in range(max_sequence_length):\n",
        "        #Apply padding\n",
        "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>')[1])\n",
        "    return tokenized_training_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSRZXzBssa-a"
      },
      "outputs": [],
      "source": [
        "# Runs the small autoregressive language model using PyTorch\n",
        "class LanguageModelRunner(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def run_model(self):\n",
        "        model_name = 'decoder_only_small_4AH_256EMB_3LYR'\n",
        "          \n",
        "        # Create the tokenizer\n",
        "        tokenizer = Tokenizer()\n",
        "\n",
        "        embedding_dimension = 256\n",
        "        max_sequence_length = 128\n",
        "        vocab_size = tokenizer.size()\n",
        "        dataset_size = 250000\n",
        "\n",
        "        # Create the model\n",
        "        model = AutoregressiveWrapper(LanguageModel(\n",
        "            embedding_dimension=embedding_dimension,\n",
        "            vocab_size=vocab_size,\n",
        "            number_of_heads=4,\n",
        "            number_of_layers=3,\n",
        "            dropout_rate=0.1,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )).to(get_device())\n",
        "\n",
        "        # Create the training data\n",
        "        with open('raw_corpus.txt', 'r', encoding=\"utf-8-sig\") as f:\n",
        "            data = f.readlines(dataset_size) #[next(f).rstrip() for x in range(250000)]\n",
        "\n",
        "        data = [line.rstrip() for line in data]\n",
        "        dataset = pd.DataFrame(data, columns=['text'])\n",
        "        dataset = dataset[dataset[\"text\"].str.strip().astype(bool)]\n",
        "        training_data = ' '.join(dataset[\"text\"].tolist())\n",
        "\n",
        "        del dataset, data\n",
        "\n",
        "        tokenized_and_padded_training_data = preprocess_training_data(max_sequence_length, tokenizer, training_data)\n",
        "        sequences = generate_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
        "\n",
        "\n",
        "        # Train the model\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "        trainer = Trainer(model, tokenizer, optimizer)\n",
        "        loss_per_epoch = trainer.train(sequences, epochs=20, batch_size=32)\n",
        "\n",
        "        # Plotting the loss per epoch in log scale\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        ax.plot(loss_per_epoch, color='blue')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_title('Training Loss over Epochs', fontsize=16, fontweight='bold')\n",
        "        ax.set_xlabel('Epoch', fontsize=14)\n",
        "        ax.set_ylabel('Loss', fontsize=14)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        plt.show()\n",
        "\n",
        "        model.save_checkpoint(model_name)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "8LCMNzgksa-a",
        "outputId": "d60656dd-1c6f-48e5-c67a-2a9c3596ed8f"
      },
      "outputs": [],
      "source": [
        "LanguageModelRunner().run_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVNEUZFYsa-a",
        "outputId": "e7aa767c-9cf4-45e9-950c-970dccbdd702"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dimension = 256 \n",
        "max_sequence_length = 128\n",
        "vocab_size = tokenizer.size()\n",
        "number_of_heads = 4 \n",
        "number_of_layers = 3 \n",
        "\n",
        "# Initialize model\n",
        "model = AutoregressiveWrapper(LanguageModel(\n",
        "            embedding_dimension = embedding_dimension,\n",
        "            max_sequence_length = max_sequence_length,\n",
        "            number_of_heads = number_of_heads, \n",
        "            number_of_layers = number_of_layers,\n",
        "            dropout_rate = 0.1,\n",
        "            vocab_size = vocab_size\n",
        "        )).to(get_device())\n",
        "\n",
        "# load the checkpoint into the model\n",
        "model.load_checkpoint('decoder_only_small_4AH_256EMB_3LYR')\n",
        "\n",
        "generated_sentence_length = 50\n",
        "generator = Generator(model, tokenizer)\n",
        "generated_text = generator.generate(\n",
        "    generated_sentence_length=generated_sentence_length,\n",
        "    prompt=\"Quiero ir a \",\n",
        "    padding_token=tokenizer.character_to_token('<pad>')[1]\n",
        ")\n",
        "\n",
        "print(generated_text.replace('<pad>', ''))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b5ea09fb112f6f6b890abad0863e7bd5ff7cc7157255d224f3ac7f70a6747825"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
